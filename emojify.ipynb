{"cells":[{"metadata":{},"cell_type":"markdown","source":"CNN for Emotion recognition from images"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom keras import backend as K \nimport warnings \nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Input,Add,Dense,Dropout,Activation,Flatten,ZeroPadding2D,BatchNormalization,GlobalMaxPooling2D,Conv2D,AveragePooling2D,MaxPooling2D,Lambda","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model,load_model,Sequential\nfrom keras.utils import layer_utils,plot_model\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom IPython.display import SVG, Image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_images(plt):\n    img_size = 48\n    plt.figure(0, figsize=(15,12))\n    ctr = 0\n    for expression in os.listdir(\"../input/fer2013/test\"):\n        for i in range(1,6):\n            ctr += 1\n            plt.subplot(7,5,ctr)\n            img = load_img(\"../input/fer2013/test/\" + expression + \"/\" +os.listdir(\"../input/fer2013/test/\" + expression)[i], target_size=(img_size, img_size))\n            plt.imshow(img)\n\n    plt.tight_layout()\n    return plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_images(plt).show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = {}\nfor expression in os.listdir(\"../input/fer2013/train/\"):\n    directory = \"../input/fer2013/train/\" + expression\n    df[expression] = len(os.listdir(directory))\ndf = pd.DataFrame(df, index=[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_size = 48\nbatch_size = 64\n\ntrain_datagen = ImageDataGenerator(rescale=1.0/255.0, horizontal_flip=True)\ntrain_datagen = ImageDataGenerator(rescale=1./255,rotation_range=40,horizontal_flip=True)\ntest_datagen = ImageDataGenerator(rescale=1.0/255.0)\n\ntrain = train_datagen.flow_from_directory(\"../input/fer2013/train/\", target_size = (img_size, img_size),\n                                             color_mode = 'grayscale',\n                                             batch_size = batch_size,\n                                             class_mode = 'categorical',\n                                             shuffle = True)\n\ntest = test_datagen.flow_from_directory(\"../input/fer2013/test/\", target_size = (img_size, img_size),\n                                             color_mode = 'grayscale',\n                                             batch_size = batch_size,\n                                             class_mode = 'categorical',\n                                             shuffle = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    model = Sequential()\n    model.add(Conv2D(64, (3, 3), activation='relu',\n                      padding='same',\n                      name='block1_conv1',input_shape=(48,48,1)))\n    model.add(Conv2D(64, (3, 3),\n                      activation='relu',\n                      padding='same',\n                      name='block1_conv2'))\n    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool'))\n\n    # Block 2\n    model.add(Conv2D(128, (3, 3),\n                      activation='relu',\n                      padding='same',\n                      name='block2_conv1'))\n    model.add(Conv2D(128, (3, 3),\n                      activation='relu',\n                      padding='same',\n                      name='block2_conv2'))\n    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool'))\n\n    # Block 3\n    model.add(Conv2D(256, (3, 3),\n                      activation='relu',\n                      padding='same',\n                      name='block3_conv1'))\n    model.add(Conv2D(256, (3, 3),\n                      activation='relu',\n                      padding='same',\n                      name='block3_conv2'))\n    model.add(Conv2D(256, (3, 3),\n                      activation='relu',\n                      padding='same',\n                      name='block3_conv3'))\n    model.add(Conv2D(256, (3, 3),\n                      activation='relu',\n                      padding='same',\n                      name='block3_conv4'))\n    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool'))\n\n    # Block 4\n    model.add(Conv2D(512, (3, 3),\n                      activation='relu',\n                      padding='same',\n                      name='block4_conv1'))\n    model.add(Conv2D(512, (3, 3),\n                      activation='relu',\n                      padding='same',\n                      name='block4_conv2'))\n    model.add(Conv2D(512, (3, 3),\n                      activation='relu',\n                      padding='same',\n                      name='block4_conv3'))\n    model.add(Conv2D(512, (3, 3),\n                      activation='relu',\n                      padding='same',\n                      name='block4_conv4'))\n    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool'))\n\n    # Block 5\n    model.add(Conv2D(512, (3, 3),\n                      activation='relu',\n                      padding='same',\n                      name='block5_conv1'))\n    model.add(Conv2D(512, (3, 3),\n                      activation='relu',\n                      padding='same',\n                      name='block5_conv2'))\n    model.add(Conv2D(512, (3, 3),\n                      activation='relu',\n                      padding='same',\n                      name='block5_conv3'))\n    model.add(Conv2D(512, (3, 3),\n                      activation='relu',\n                      padding='same',\n                      name='block5_conv4'))\n    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool'))\n    \n    model.add(Flatten())\n    model.add(Dense(1024, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(7, activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.0001, decay=1e-6),metrics=['accuracy'])\n#model_info = model.fit_generator(train,steps_per_epoch=28709 // 64,epochs=50, validation_data=test,validation_steps=7178 // 64)\nmodel.fit(train,steps_per_epoch=28709 // 64,epochs=50, validation_data=test,validation_steps=7178 // 64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save_weights('model.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Imported Code for detecting faces"},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv2.ocl.setUseOpenCL(False)\nemotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\ncap = cv2.VideoCapture(0)\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n    bounding_box = cv2.CascadeClassifier('/home/shivam/.local/lib/python3.6/site-packages/cv2/data/haarcascade_frontalface_default.xml')\n    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2gray_frame)\n    num_faces = bounding_box.detectMultiScale(gray_frame,scaleFactor=1.3, minNeighbors=5)\n    for (x, y, w, h) in num_faces:\n        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n        emotion_prediction = emotion_model.predict(cropped_img)\n        maxindex = int(np.argmax(emotion_prediction))\n        cv2.putText(frame, emotion_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n    cv2.imshow('Video', cv2.resize(frame,(1200,860),interpolation = cv2.INTER_CUBIC))\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        cap.release()\n        cv2.destroyAllWindows()\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tkinter as tk\nfrom tkinter import *\nimport cv2\nfrom PIL import Image, ImageTk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv2.ocl.setUseOpenCL(False)\nemotion_dict = {0: \"   Angry   \", 1: \"Disgusted\", 2: \"  Fearful  \", 3: \"   Happy   \", 4: \"  Neutral  \", 5: \"    Sad    \", 6: \"Surprised\"}\nemoji_dist={0:\"../input/emojis/anrgy.jpg\",2:\"../input/emojis/disgusted.jpg\",2:\"../input/emojis/fearful.jpg\",3:\"../input/emojis/happy.jpg\",\n            4:\"../input/emojish/neutral.jpg\",5:\"../input/emojis/sad.jpg\",6:\"../input/emojis/surprise.jpg\"}\nglobal last_frame1                                    \nlast_frame1 = np.zeros((480, 640, 3), dtype=np.uint8)\nglobal cap1\nshow_text=[0]\ndef show_vid():      \n    cap1 = cv2.VideoCapture(0)                                 \n    if not cap1.isOpened():                             \n        print(\"cant open the camera1\")\n    flag1, frame1 = cap1.read()\n    frame1 = cv2.resize(frame1,(600,500))\n    bounding_box = cv2.CascadeClassifier('/home/shivam/.local/lib/python3.6/site-packages/cv2/data/haarcascade_frontalface_default.xml')\n    gray_frame = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n    num_faces = bounding_box.detectMultiScale(gray_frame,scaleFactor=1.3, minNeighbors=5)\n    for (x, y, w, h) in num_faces:\n        cv2.rectangle(frame1, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n        prediction = emotion_model.predict(cropped_img)\n        \n        maxindex = int(np.argmax(prediction))\n        cv2.putText(frame1, emotion_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n        show_text[0]=maxindex\n    if flag1 is None:\n        print (\"Major error!\")\n    elif flag1:\n        global last_frame1\n        last_frame1 = frame1.copy()\n        pic = cv2.cvtColor(last_frame1, cv2.COLOR_BGR2RGB)     \n        img = Image.fromarray(pic)\n        imgtk = ImageTk.PhotoImage(image=img)\n        lmain.imgtk = imgtk\n        lmain.configure(image=imgtk)\n        lmain.after(10, show_vid)\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        exit()\ndef show_vid2():\n    frame2=cv2.imread(emoji_dist[show_text[0]])\n    pic2=cv2.cvtColor(frame2,cv2.COLOR_BGR2RGB)\n    img2=Image.fromarray(frame2)\n    imgtk2=ImageTk.PhotoImage(image=img2)\n    lmain2.imgtk2=imgtk2\n    lmain3.configure(text=emotion_dict[show_text[0]],font=('arial',45,'bold'))\n    \n    lmain2.configure(image=imgtk2)\n    lmain2.after(10, show_vid2)\nif __name__ == '__main__':\n    root=tk.Tk()   \n    img = ImageTk.PhotoImage(Image.open(\"logo.png\"))\n    heading = Label(root,image=img,bg='black')\n    \n    heading.pack() \n    heading2=Label(root,text=\"Photo to Emoji\",pady=20, font=('arial',45,'bold'),bg='black',fg='#CDCDCD')                                 \n    \n    heading2.pack()\n    lmain = tk.Label(master=root,padx=50,bd=10)\n    lmain2 = tk.Label(master=root,bd=10)\n    lmain3=tk.Label(master=root,bd=10,fg=\"#CDCDCD\",bg='black')\n    lmain.pack(side=LEFT)\n    lmain.place(x=50,y=250)\n    lmain3.pack()\n    lmain3.place(x=960,y=250)\n    lmain2.pack(side=RIGHT)\n    lmain2.place(x=900,y=350)\n    \n    root.title(\"Photo To Emoji\")            \n    root.geometry(\"1400x900+100+10\") \n    root['bg']='black'\n    exitbutton = Button(root, text='Quit',fg=\"red\",command=root.destroy,font=('arial',25,'bold')).pack(side = BOTTOM)\n    show_vid()\n    show_vid2()\n    root.mainloop()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}